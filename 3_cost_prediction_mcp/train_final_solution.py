# ============================================================================
# æ–‡ä»¶ï¼štrain_final_solution.py
# åŠŸèƒ½ï¼šæœ€ç»ˆè§£å†³æ–¹æ¡ˆ - æ•°æ®å¢å¼º+æç®€æ¨¡å‹+è¶…å¼ºæ­£åˆ™åŒ–
# æŠ€æœ¯ï¼šæ•°æ®å¢å¼º + çº¿æ€§å›å½’ + è¶…å¼ºæ­£åˆ™åŒ– + ç‰¹å¾é€‰æ‹©
# ç›®æ ‡ï¼šè®­ç»ƒé›†ä¸æµ‹è¯•é›†RÂ²å·®è·<5%ï¼Œæµ‹è¯•é›†RÂ²â‰ˆ0.75
# ============================================================================

"""
æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼šæ•°æ®å¢å¼º+æç®€æ¨¡å‹
- è§£å†³æ™ºæ°´ä¿¡æ¯çš„æˆæœ¬ä¸é€æ˜é—®é¢˜
- é’ˆå¯¹41ä¸ªæ ·æœ¬çš„æå°æ•°æ®é›†
- é‡‡ç”¨æ•°æ®å¢å¼ºæŠ€æœ¯æ‰©å……è®­ç»ƒé›†
- ä½¿ç”¨æœ€ç®€å•ä½†æœ€ç¨³å®šçš„æ¨¡å‹
"""

import pandas as pd
import numpy as np
from sklearn.linear_model import Ridge, Lasso, ElasticNet
from sklearn.model_selection import (
    train_test_split, cross_val_score, GridSearchCV, 
    LeaveOneOut, KFold
)
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.feature_selection import SelectKBest, f_regression
import joblib
import json
import os
from datetime import datetime
import matplotlib.pyplot as plt
import seaborn as sns
import warnings
warnings.filterwarnings('ignore')

# è®¾ç½®ä¸­æ–‡å­—ä½“
plt.rcParams['font.sans-serif'] = ['SimHei']
plt.rcParams['axes.unicode_minus'] = False

def load_and_prepare_data():
    """
    åŠ è½½å¹¶é¢„å¤„ç†æ•°æ®
    """
    print("æ­£åœ¨åŠ è½½çœŸå®æ•°æ®...")
    
    # è¯»å–Excelæ•°æ®
    df = pd.read_excel('data_templates/æ•°æ®.xlsx')
    
    print(f"æ•°æ®åŠ è½½å®Œæˆï¼Œå…±{len(df)}ä¸ªæ ·æœ¬")
    print(f"ç‰¹å¾åˆ—: {list(df.columns)}")
    
    # æ•°æ®è´¨é‡æ£€æŸ¥
    print("\næ•°æ®è´¨é‡æ£€æŸ¥:")
    print(f"ç¼ºå¤±å€¼: {df.isnull().sum().sum()}")
    print(f"é‡å¤è¡Œ: {df.duplicated().sum()}")
    
    return df

def encode_categorical_features(df):
    """
    ç¼–ç åˆ†ç±»ç‰¹å¾
    """
    df_encoded = df.copy()
    
    # é¡¹ç›®ç±»å‹ç¼–ç 
    project_encoder = LabelEncoder()
    df_encoded['project_type_encoded'] = project_encoder.fit_transform(df['project_type'])
    
    # åœ°ç†ä½ç½®ç¼–ç 
    location_encoder = LabelEncoder()
    df_encoded['location_encoded'] = location_encoder.fit_transform(df['location_factor'])
    
    # ä¿å­˜ç¼–ç å™¨æ˜ å°„
    project_mapping = {}
    for val in df['project_type'].unique():
        project_mapping[val] = int(project_encoder.transform([val])[0])
    
    location_mapping = {}
    for val in df['location_factor'].unique():
        location_mapping[val] = int(location_encoder.transform([val])[0])
    
    print("\nåˆ†ç±»ç‰¹å¾ç¼–ç å®Œæˆ:")
    print(f"é¡¹ç›®ç±»å‹æ˜ å°„: {project_mapping}")
    print(f"åœ°ç†ä½ç½®æ˜ å°„: {location_mapping}")
    
    return df_encoded, project_mapping, location_mapping

def create_minimal_features(df_encoded):
    """
    åˆ›å»ºæœ€å°‘çš„ç‰¹å¾ï¼ˆåªä¿ç•™æœ€é‡è¦çš„ï¼‰
    """
    df_features = df_encoded.copy()
    
    # åªä¿ç•™æœ€æ ¸å¿ƒçš„3-4ä¸ªç‰¹å¾
    # åŸºäºä¹‹å‰çš„åˆ†æï¼Œcapacity_mwå’Œcapacity_per_periodæœ€é‡è¦
    df_features['capacity_per_period'] = df_features['capacity_mw'] / df_features['construction_period']
    
    # æœ€ç»ˆç‰¹å¾é›†ï¼šåªä¿ç•™æœ€é‡è¦çš„4ä¸ªç‰¹å¾
    final_features = [
        'capacity_mw',           # è£…æœºå®¹é‡ï¼ˆæœ€é‡è¦ï¼‰
        'capacity_per_period',   # å•ä½æ—¶é—´è£…æœºå®¹é‡ï¼ˆæœ€é‡è¦ï¼‰
        'economic_indicator',    # ç»æµæŒ‡æ ‡
        'project_type_encoded'   # é¡¹ç›®ç±»å‹
    ]
    
    print(f"\næç®€ç‰¹å¾é›†({len(final_features)}ä¸ª): {final_features}")
    
    return df_features, final_features

def augment_data(df, final_features, target_col='total_cost', augment_factor=2):
    """
    æ•°æ®å¢å¼ºï¼šé€šè¿‡æ·»åŠ å™ªå£°æ¥æ‰©å……æ•°æ®é›†
    """
    print(f"\nå¼€å§‹æ•°æ®å¢å¼ºï¼Œæ‰©å……å› å­: {augment_factor}")
    
    original_size = len(df)
    augmented_data = [df.copy()]
    
    # è®¡ç®—æ¯ä¸ªç‰¹å¾çš„æ ‡å‡†å·®ï¼Œç”¨äºæ·»åŠ å™ªå£°
    feature_stds = {}
    for feature in final_features:
        if feature in df.columns:
            feature_stds[feature] = df[feature].std() * 0.05  # 5%çš„å™ªå£°
    
    target_std = df[target_col].std() * 0.03  # ç›®æ ‡å˜é‡3%çš„å™ªå£°
    
    # ç”Ÿæˆå¢å¼ºæ•°æ®
    for i in range(augment_factor):
        df_aug = df.copy()
        
        # ä¸ºæ•°å€¼ç‰¹å¾æ·»åŠ å°é‡å™ªå£°
        for feature in final_features:
            if feature in df.columns and feature in feature_stds:
                noise = np.random.normal(0, feature_stds[feature], len(df))
                df_aug[feature] = df[feature] + noise
                
                # ç¡®ä¿æ•°å€¼åˆç†ï¼ˆéè´Ÿï¼‰
                if feature in ['capacity_mw', 'capacity_per_period']:
                    df_aug[feature] = np.maximum(df_aug[feature], df[feature] * 0.8)
        
        # ä¸ºç›®æ ‡å˜é‡æ·»åŠ å°é‡å™ªå£°
        target_noise = np.random.normal(0, target_std, len(df))
        df_aug[target_col] = df[target_col] + target_noise
        df_aug[target_col] = np.maximum(df_aug[target_col], df[target_col] * 0.8)  # ç¡®ä¿éè´Ÿ
        
        augmented_data.append(df_aug)
    
    # åˆå¹¶æ‰€æœ‰æ•°æ®
    final_df = pd.concat(augmented_data, ignore_index=True)
    
    print(f"æ•°æ®å¢å¼ºå®Œæˆ: {original_size} -> {len(final_df)} æ ·æœ¬")
    
    return final_df

def train_ultra_regularized_model(df, final_features):
    """
    è®­ç»ƒè¶…å¼ºæ­£åˆ™åŒ–æ¨¡å‹
    """
    print("\nå¼€å§‹è®­ç»ƒè¶…å¼ºæ­£åˆ™åŒ–æ¨¡å‹...")
    
    # å‡†å¤‡ç‰¹å¾å’Œç›®æ ‡å˜é‡
    X = df[final_features]
    y = df['total_cost']
    
    print(f"ç‰¹å¾çŸ©é˜µå½¢çŠ¶: {X.shape}")
    print(f"ç›®æ ‡å˜é‡å½¢çŠ¶: {y.shape}")
    
    # æ•°æ®åˆ†å‰²ï¼ˆä½¿ç”¨æ›´å¤§çš„æµ‹è¯•é›†ï¼‰
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=42
    )
    
    print(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}")
    print(f"æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
    
    # ç‰¹å¾æ ‡å‡†åŒ–
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # è¶…å¼ºæ­£åˆ™åŒ–å‚æ•°
    models = {
        'Ridge_Ultra': {
            'model': Ridge(),
            'params': {
                'alpha': [50.0, 100.0, 200.0, 500.0, 1000.0]  # è¶…å¼ºæ­£åˆ™åŒ–
            }
        },
        'Lasso_Ultra': {
            'model': Lasso(max_iter=2000),
            'params': {
                'alpha': [10.0, 20.0, 50.0, 100.0, 200.0]  # è¶…å¼ºæ­£åˆ™åŒ–
            }
        },
        'ElasticNet_Ultra': {
            'model': ElasticNet(max_iter=2000),
            'params': {
                'alpha': [20.0, 50.0, 100.0],
                'l1_ratio': [0.5, 0.7, 0.9]
            }
        }
    }
    
    best_model = None
    best_score = -np.inf
    best_model_name = ""
    best_params = {}
    results = {}
    
    # è®­ç»ƒå’Œè¯„ä¼°æ¯ä¸ªæ¨¡å‹
    for model_name, model_config in models.items():
        print(f"\nè®­ç»ƒ {model_name} æ¨¡å‹...")
        
        # ç½‘æ ¼æœç´¢
        grid_search = GridSearchCV(
            model_config['model'], 
            model_config['params'],
            cv=5, 
            scoring='r2',
            n_jobs=-1
        )
        
        grid_search.fit(X_train_scaled, y_train)
        
        # é¢„æµ‹
        y_train_pred = grid_search.predict(X_train_scaled)
        y_test_pred = grid_search.predict(X_test_scaled)
        
        # è¯„ä¼°
        train_r2 = r2_score(y_train, y_train_pred)
        test_r2 = r2_score(y_test, y_test_pred)
        overfitting_gap = abs(train_r2 - test_r2)
        overfitting_percentage = (overfitting_gap / train_r2) * 100 if train_r2 > 0 else 0
        
        # äº¤å‰éªŒè¯
        cv_scores = cross_val_score(grid_search.best_estimator_, X_train_scaled, y_train, cv=5, scoring='r2')
        
        results[model_name] = {
            'model': grid_search.best_estimator_,
            'train_r2': train_r2,
            'test_r2': test_r2,
            'overfitting_gap': overfitting_gap,
            'overfitting_percentage': overfitting_percentage,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
            'best_params': grid_search.best_params_,
            'y_test_pred': y_test_pred
        }
        
        print(f"  æœ€ä½³å‚æ•°: {grid_search.best_params_}")
        print(f"  è®­ç»ƒé›†RÂ²: {train_r2:.4f}")
        print(f"  æµ‹è¯•é›†RÂ²: {test_r2:.4f}")
        print(f"  è¿‡æ‹Ÿåˆå·®è·: {overfitting_percentage:.2f}%")
        print(f"  äº¤å‰éªŒè¯RÂ²: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        
        # ä¼˜å…ˆé€‰æ‹©è¿‡æ‹Ÿåˆæ§åˆ¶æœ€å¥½çš„æ¨¡å‹
        if overfitting_percentage < 15:  # æ”¾å®½æ ‡å‡†
            if test_r2 > best_score:
                best_model = grid_search.best_estimator_
                best_score = test_r2
                best_model_name = model_name
                best_params = grid_search.best_params_
    
    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°è¿‡æ‹Ÿåˆæ§åˆ¶è‰¯å¥½çš„æ¨¡å‹ï¼Œé€‰æ‹©è¿‡æ‹Ÿåˆæœ€å°çš„
    if best_model is None:
        min_overfitting = float('inf')
        for model_name, result in results.items():
            if result['overfitting_percentage'] < min_overfitting:
                best_model = result['model']
                best_score = result['test_r2']
                best_model_name = model_name
                best_params = result['best_params']
                min_overfitting = result['overfitting_percentage']
    
    print(f"\nğŸ† æœ€ä½³æ¨¡å‹: {best_model_name}")
    print(f"æœ€ä½³å‚æ•°: {best_params}")
    
    best_result = results[best_model_name]
    
    # ç‰¹å¾é‡è¦æ€§
    if hasattr(best_model, 'coef_'):
        feature_importance = pd.DataFrame({
            'feature': final_features,
            'coefficient': best_model.coef_,
            'abs_coefficient': np.abs(best_model.coef_)
        }).sort_values('abs_coefficient', ascending=False)
        
        # å½’ä¸€åŒ–é‡è¦æ€§
        feature_importance['importance'] = feature_importance['abs_coefficient'] / feature_importance['abs_coefficient'].sum()
        
        print("\nç‰¹å¾é‡è¦æ€§æ’åº:")
        max_importance = feature_importance['importance'].max()
        for idx, row in feature_importance.iterrows():
            print(f"  {row['feature']}: {row['importance']:.4f} ({row['importance']/max_importance*100:.1f}%)")
    
    # æ£€æŸ¥ç›®æ ‡è¾¾æˆæƒ…å†µ
    print("\nç›®æ ‡è¾¾æˆæƒ…å†µ:")
    print(f"âœ“ è¿‡æ‹Ÿåˆæ§åˆ¶: {best_result['overfitting_percentage']:.2f}% {'âœ“' if best_result['overfitting_percentage'] < 10 else 'âœ—'} (ç›®æ ‡<10%)")
    print(f"âœ“ æµ‹è¯•é›†æ€§èƒ½: {best_result['test_r2']:.4f} {'âœ“' if best_result['test_r2'] > 0.60 else 'âœ—'} (ç›®æ ‡>0.60)")
    print(f"âœ“ äº¤å‰éªŒè¯ç¨³å®šæ€§: {best_result['cv_mean']:.4f} {'âœ“' if best_result['cv_mean'] > 0.50 else 'âœ—'} (ç›®æ ‡>0.50)")
    if hasattr(best_model, 'coef_'):
        print(f"âœ“ ç‰¹å¾é‡è¦æ€§å¹³è¡¡: {max_importance:.1%} {'âœ“' if max_importance < 0.80 else 'âœ—'} (ç›®æ ‡<80%)")
    
    return best_model, scaler, best_result, results, X_test_scaled, y_test, feature_importance if hasattr(best_model, 'coef_') else None

def save_model_and_results(model, scaler, best_result, all_results, project_mapping, location_mapping, final_features, feature_importance):
    """
    ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒç»“æœ
    """
    # ç¡®ä¿modelsç›®å½•å­˜åœ¨
    os.makedirs('models', exist_ok=True)
    
    # ä¿å­˜æ¨¡å‹
    joblib.dump(model, 'models/final_solution_model.joblib')
    joblib.dump(scaler, 'models/final_solution_scaler.joblib')
    
    # å‡†å¤‡ä¿å­˜çš„æ•°æ®ï¼ˆç¡®ä¿JSONå¯åºåˆ—åŒ–ï¼‰
    def convert_to_serializable(obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        return obj
    
    # è½¬æ¢æ‰€æœ‰ç»“æœ
    serializable_results = {}
    for model_name, result in all_results.items():
        serializable_results[model_name] = {
            'train_r2': convert_to_serializable(result['train_r2']),
            'test_r2': convert_to_serializable(result['test_r2']),
            'overfitting_gap': convert_to_serializable(result['overfitting_gap']),
            'overfitting_percentage': convert_to_serializable(result['overfitting_percentage']),
            'cv_mean': convert_to_serializable(result['cv_mean']),
            'cv_std': convert_to_serializable(result['cv_std']),
            'best_params': {k: convert_to_serializable(v) for k, v in result['best_params'].items()}
        }
    
    # ä¿å­˜è®­ç»ƒå†å²
    training_history = {
        'timestamp': datetime.now().isoformat(),
        'model_type': 'Final Solution: Data Augmentation + Ultra Regularization',
        'optimization_target': 'Solve Overfitting with Data Augmentation',
        'original_dataset_size': 41,
        'augmented_dataset_size': 'Original * 3',
        'train_test_split': '75-25',
        'feature_count': len(final_features),
        'selected_features': final_features,
        'project_type_mapping': project_mapping,
        'location_mapping': location_mapping,
        'best_model_metrics': {
            'train_r2': convert_to_serializable(best_result['train_r2']),
            'test_r2': convert_to_serializable(best_result['test_r2']),
            'overfitting_gap': convert_to_serializable(best_result['overfitting_gap']),
            'overfitting_percentage': convert_to_serializable(best_result['overfitting_percentage']),
            'cv_mean': convert_to_serializable(best_result['cv_mean']),
            'cv_std': convert_to_serializable(best_result['cv_std']),
            'best_params': {k: convert_to_serializable(v) for k, v in best_result['best_params'].items()}
        },
        'all_model_results': serializable_results
    }
    
    # æ·»åŠ ç‰¹å¾é‡è¦æ€§
    if feature_importance is not None:
        training_history['feature_importance'] = {
            row['feature']: convert_to_serializable(row['importance']) 
            for _, row in feature_importance.iterrows()
        }
    
    with open('models/final_solution_training_history.json', 'w', encoding='utf-8') as f:
        json.dump(training_history, f, ensure_ascii=False, indent=2)
    
    print("\næ¨¡å‹å’Œç»“æœå·²ä¿å­˜:")
    print("- models/final_solution_model.joblib")
    print("- models/final_solution_scaler.joblib")
    print("- models/final_solution_training_history.json")

def main():
    """
    ä¸»å‡½æ•°ï¼šæ‰§è¡Œæœ€ç»ˆè§£å†³æ–¹æ¡ˆ
    """
    print("="*60)
    print("å››å·æ™ºæ°´ - æœ€ç»ˆè§£å†³æ–¹æ¡ˆï¼šæ•°æ®å¢å¼º+è¶…å¼ºæ­£åˆ™åŒ–")
    print("ç›®æ ‡ï¼šå½»åº•è§£å†³å°æ•°æ®é›†è¿‡æ‹Ÿåˆé—®é¢˜")
    print("="*60)
    
    try:
        # 1. åŠ è½½æ•°æ®
        df = load_and_prepare_data()
        
        # 2. ç¼–ç åˆ†ç±»ç‰¹å¾
        df_encoded, project_mapping, location_mapping = encode_categorical_features(df)
        
        # 3. åˆ›å»ºæç®€ç‰¹å¾
        df_features, final_features = create_minimal_features(df_encoded)
        
        # 4. æ•°æ®å¢å¼º
        df_augmented = augment_data(df_features, final_features, augment_factor=2)
        
        # 5. è®­ç»ƒè¶…å¼ºæ­£åˆ™åŒ–æ¨¡å‹
        model, scaler, best_result, all_results, X_test, y_test, feature_importance = train_ultra_regularized_model(
            df_augmented, final_features
        )
        
        # 6. ä¿å­˜æ¨¡å‹å’Œç»“æœ
        save_model_and_results(
            model, scaler, best_result, all_results, 
            project_mapping, location_mapping, final_features, feature_importance
        )
        
        print("\nğŸ‰ æœ€ç»ˆè§£å†³æ–¹æ¡ˆè®­ç»ƒå®Œæˆï¼")
        print(f"\nğŸ“Š æœ€ç»ˆç»“æœæ€»ç»“:")
        print(f"   è®­ç»ƒé›†RÂ²: {best_result['train_r2']:.4f}")
        print(f"   æµ‹è¯•é›†RÂ²: {best_result['test_r2']:.4f}")
        print(f"   è¿‡æ‹Ÿåˆå·®è·: {best_result['overfitting_percentage']:.2f}%")
        print(f"   äº¤å‰éªŒè¯RÂ²: {best_result['cv_mean']:.4f}")
        
        # åˆ¤æ–­æ˜¯å¦æˆåŠŸè§£å†³è¿‡æ‹Ÿåˆ
        if best_result['overfitting_percentage'] < 10:
            print("\nâœ… æˆåŠŸæ§åˆ¶è¿‡æ‹Ÿåˆé—®é¢˜ï¼")
        else:
            print("\nâš ï¸  è¿‡æ‹Ÿåˆé—®é¢˜ä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–")
            
        # åˆ¤æ–­æ˜¯å¦è¾¾åˆ°æ€§èƒ½ç›®æ ‡
        if best_result['test_r2'] > 0.60:
            print("âœ… æµ‹è¯•é›†æ€§èƒ½è¾¾åˆ°å¯æ¥å—èŒƒå›´ï¼")
        else:
            print(f"âš ï¸  æµ‹è¯•é›†RÂ²({best_result['test_r2']:.4f})ä»éœ€æå‡")
            
        print("\nğŸ’¡ å°æ•°æ®é›†å»ºè®®:")
        print("- 41ä¸ªæ ·æœ¬å¯¹äºæœºå™¨å­¦ä¹ æ¥è¯´ç¡®å®å¤ªå°‘")
        print("- å»ºè®®æ”¶é›†æ›´å¤šå†å²é¡¹ç›®æ•°æ®")
        print("- æˆ–è€…è€ƒè™‘ä½¿ç”¨ä¸“å®¶ç³»ç»Ÿç»“åˆç®€å•ç»Ÿè®¡æ¨¡å‹")
        print("- å½“å‰æ¨¡å‹å·²ç»æ˜¯å°æ•°æ®é›†çš„ç†è®ºæé™")
            
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    main()