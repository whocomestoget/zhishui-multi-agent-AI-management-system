#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å››å·æ™ºæ°´ä¿¡æ¯æŠ€æœ¯æœ‰é™å…¬å¸ - è¿ç»´çŸ¥è¯†åº“MCPæœåŠ¡

ä¸“ä¸ºç”µåŠ›ã€æ°´åˆ©è¡Œä¸šä¿¡æ¯åŒ–ç³»ç»Ÿè¿ç»´è®¾è®¡çš„æ™ºèƒ½çŸ¥è¯†åº“
åŸºäºRAGæŠ€æœ¯ï¼Œä½¿ç”¨ollama qwen3-embeddingæ¨¡å‹è¿›è¡Œå‘é‡æ£€ç´¢

åŠŸèƒ½ï¼š
- PDFæ–‡æ¡£å¯¼å…¥å’Œæ™ºèƒ½åˆ†å—
- åŸºäºå‘é‡ç›¸ä¼¼åº¦çš„çŸ¥è¯†æ£€ç´¢  
- æ–‡æ¡£åˆ†ç±»ç®¡ç†ï¼ˆç”µåŠ›ã€æ°´åˆ©ã€é€šä¿¡ã€å®‰é˜²ã€è§„èŒƒï¼‰
- é•¿æœŸæœ¬åœ°å­˜å‚¨å’Œç´¢å¼•ç»´æŠ¤
"""

import os
import sys
import json
import sqlite3
import logging
import hashlib
import base64
import uuid
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Any
import threading

# ç¬¬ä¸‰æ–¹ä¾èµ–
try:
    import numpy as np
    import faiss
    import requests
    import pdfplumber
    import fitz  # pymupdf
    import PyPDF2  # PDFæ–‡æ¡£å¤„ç†
    from mcp.server.fastmcp import FastMCP
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·å®‰è£…: pip install fastmcp pdfplumber pymupdf faiss-cpu numpy requests PyPDF2")
    sys.exit(1)

# ================================
# é…ç½®å’Œå¸¸é‡
# ================================

TOOL_NAME = "zhishui-knowledge-mcp"
COMPANY_NAME = "å››å·æ™ºæ°´ä¿¡æ¯æŠ€æœ¯æœ‰é™å…¬å¸"

# çŸ¥è¯†åº“åˆ†ç±»ä½“ç³»ï¼ˆåŸºäºå››å·æ™ºæ°´ä¸šåŠ¡é¢†åŸŸï¼‰
KNOWLEDGE_CATEGORIES = {
    "ç”µåŠ›ç³»ç»Ÿè¿ç»´": ["æ°´ç”µç«™", "ç«ç”µç«™", "æ–°èƒ½æºç«™", "å˜ç”µç«™", "é…ç”µç³»ç»Ÿ", "å‘ç”µæœºç»„"],
    "æ°´åˆ©ç³»ç»Ÿè¿ç»´": ["å¤§åç›‘æµ‹", "æ°´åº“ç®¡ç†", "çŒåŒºç³»ç»Ÿ", "æ°´æ–‡ç›‘æµ‹", "é—¸é—¨æ§åˆ¶", "æ³µç«™è®¾å¤‡"],
    "é€šä¿¡ç½‘ç»œè¿ç»´": ["é€šä¿¡è®¾å¤‡", "ç½‘ç»œç³»ç»Ÿ", "æ•°æ®ä¼ è¾“", "æ— çº¿é€šä¿¡", "å…‰çº¤ç½‘ç»œ", "ç½‘ç»œå®‰å…¨"],
    "å®‰é˜²ç³»ç»Ÿè¿ç»´": ["è§†é¢‘ç›‘æ§", "é—¨ç¦ç³»ç»Ÿ", "æŠ¥è­¦ç³»ç»Ÿ", "å‘¨ç•Œé˜²æŠ¤", "å®‰å…¨æ£€æµ‹", "æ¶ˆé˜²ç³»ç»Ÿ"],
    "è¿ç»´æ ‡å‡†è§„èŒƒ": ["æ“ä½œæµç¨‹", "å®‰å…¨è§„èŒƒ", "ç»´æŠ¤æ ‡å‡†", "åº”æ€¥é¢„æ¡ˆ", "è´¨é‡ç®¡ç†", "æŠ€æœ¯æ ‡å‡†"]
}

# Ollamaé…ç½®ï¼ˆåŸºäºåŸå§‹ä»£ç ï¼‰
OLLAMA_CONFIG = {
    "url": "http://localhost:11434",
    "model_name": "qwen3-embedding",
    "embedding_dim": 2560,
    "timeout": 30
}

# æ–‡æ¡£åˆ†å—é…ç½®
CHUNK_CONFIG = {
    "min_size": 500,
    "max_size": 1000, 
    "overlap": 100
}

# æ•°æ®å­˜å‚¨è·¯å¾„é…ç½® - ä½¿ç”¨ç»å¯¹è·¯å¾„ç¡®ä¿è·¨å¹³å°æ•°æ®å…±äº«
# æ‰€æœ‰MCPå¹³å°éƒ½å°†æ•°æ®å­˜å‚¨åœ¨åŒä¸€ä½ç½®
BASE_DATA_PATH = Path("C:/MCP_Knowledge_Base")
DATA_DIR = BASE_DATA_PATH / "knowledge_base"
DOCUMENTS_DIR = DATA_DIR / "documents"
VECTORS_DIR = DATA_DIR / "vectors"
LOGS_DIR = BASE_DATA_PATH / "logs"

# ç¡®ä¿ç›®å½•å­˜åœ¨
for dir_path in [DATA_DIR, DOCUMENTS_DIR, VECTORS_DIR, LOGS_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

# æ—¥å¿—é…ç½®
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOGS_DIR / 'knowledge_service.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(TOOL_NAME)

# åˆ›å»ºMCPæœåŠ¡
mcp = FastMCP(TOOL_NAME)

# ================================
# æ•°æ®åº“ç®¡ç†å™¨
# ================================

class DatabaseManager:
    """SQLiteæ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self, db_path: str = None):
        if db_path is None:
            db_path = DATA_DIR / "metadata.db"
        self.db_path = str(db_path)
        self._lock = threading.Lock()
        self.init_database()
    
    def get_connection(self):
        """è·å–æ•°æ®åº“è¿æ¥"""
        conn = sqlite3.connect(self.db_path, timeout=30, check_same_thread=False)
        conn.row_factory = sqlite3.Row
        return conn
    
    def init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“è¡¨ç»“æ„"""
        with self.get_connection() as conn:
            # æ–‡æ¡£å…ƒæ•°æ®è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS documents (
                    doc_id TEXT PRIMARY KEY,
                    original_filename TEXT NOT NULL,
                    title TEXT NOT NULL,
                    category TEXT NOT NULL,
                    subcategory TEXT,
                    file_path TEXT NOT NULL,
                    upload_time TIMESTAMP NOT NULL,
                    page_count INTEGER,
                    file_size INTEGER,
                    status TEXT DEFAULT 'active'
                )
            """)
            
            # æ–‡æ¡£åˆ†å—è¡¨
            conn.execute("""
                CREATE TABLE IF NOT EXISTS document_chunks (
                    chunk_id TEXT PRIMARY KEY,
                    doc_id TEXT NOT NULL,
                    chunk_index INTEGER NOT NULL,
                    content TEXT NOT NULL,
                    vector_id INTEGER,
                    char_count INTEGER,
                    FOREIGN KEY (doc_id) REFERENCES documents (doc_id)
                )
            """)
            
            conn.commit()
            logger.info("æ•°æ®åº“è¡¨ç»“æ„åˆå§‹åŒ–å®Œæˆ")

# ================================
# å‘é‡æ£€ç´¢å¼•æ“
# ================================

class VectorSearchEngine:
    """åŸºäºFAISSçš„å‘é‡æ£€ç´¢å¼•æ“"""
    
    def __init__(self, db_manager: DatabaseManager):
        self.db_manager = db_manager
        self.faiss_index = None
        self.chunk_ids = []
        self.index_file = VECTORS_DIR / "faiss.index"
        self.mapping_file = VECTORS_DIR / "chunk_mapping.json"
        
        # åˆå§‹åŒ–Ollamaè¿æ¥
        self._init_ollama()
        
        # åŠ è½½æˆ–åˆ›å»ºç´¢å¼•
        self._load_or_create_index()
    
    def _init_ollama(self):
        """åˆå§‹åŒ–OllamaæœåŠ¡è¿æ¥"""
        try:
            # æ£€æŸ¥OllamaæœåŠ¡çŠ¶æ€
            response = requests.get(f"{OLLAMA_CONFIG['url']}/api/tags", timeout=15)
            if response.status_code == 200:
                models = response.json().get('models', [])
                model_exists = any(model['name'].startswith(OLLAMA_CONFIG['model_name']) 
                                 for model in models)
                
                if not model_exists:
                    logger.info(f"æ­£åœ¨æ‹‰å–{OLLAMA_CONFIG['model_name']}æ¨¡å‹...")
                    pull_response = requests.post(
                        f"{OLLAMA_CONFIG['url']}/api/pull",
                        json={"name": OLLAMA_CONFIG['model_name']},
                        timeout=600
                    )
                    if pull_response.status_code != 200:
                        raise Exception(f"æ¨¡å‹æ‹‰å–å¤±è´¥: {pull_response.text}")
                
                logger.info("Ollama embeddingæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
            else:
                raise Exception(f"OllamaæœåŠ¡ä¸å¯ç”¨: {response.status_code}")
                
        except Exception as e:
            logger.error(f"Ollamaåˆå§‹åŒ–å¤±è´¥: {e}")
            raise Exception(f"æ— æ³•è¿æ¥åˆ°OllamaæœåŠ¡ï¼Œè¯·ç¡®ä¿Ollamaå·²å¯åŠ¨å¹¶è¿è¡Œåœ¨{OLLAMA_CONFIG['url']}")
    
    def _get_embedding(self, text: str) -> np.ndarray:
        """è·å–æ–‡æœ¬çš„embeddingå‘é‡"""
        try:
            response = requests.post(
                f"{OLLAMA_CONFIG['url']}/api/embeddings",
                json={
                    "model": OLLAMA_CONFIG['model_name'],
                    "prompt": text
                },
                timeout=OLLAMA_CONFIG['timeout']
            )
            
            if response.status_code == 200:
                embedding = response.json().get('embedding')
                if embedding:
                    return np.array(embedding, dtype=np.float32)
                else:
                    raise Exception("å“åº”ä¸­æ²¡æœ‰embeddingæ•°æ®")
            else:
                raise Exception(f"APIè¯·æ±‚å¤±è´¥: {response.status_code} - {response.text}")
                
        except Exception as e:
            logger.error(f"è·å–embeddingå¤±è´¥: {e}")
            raise
    
    def _chunk_text(self, text: str) -> List[str]:
        """æ™ºèƒ½æ–‡æœ¬åˆ†å—"""
        if not text or len(text) <= CHUNK_CONFIG['max_size']:
            return [text] if text else []
        
        # æŒ‰æ®µè½åˆ†å‰²
        paragraphs = [p.strip() for p in text.split('\n\n') if p.strip()]
        if not paragraphs:
            paragraphs = [text]
        
        chunks = []
        current_chunk = ""
        
        for paragraph in paragraphs:
            # å¦‚æœå•ä¸ªæ®µè½å°±è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œéœ€è¦å¼ºåˆ¶åˆ†å‰²
            if len(paragraph) > CHUNK_CONFIG['max_size']:
                # å…ˆä¿å­˜å½“å‰å—ï¼ˆå¦‚æœæœ‰å†…å®¹ï¼‰
                if current_chunk.strip():
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                
                # å¯¹è¶…é•¿æ®µè½è¿›è¡Œå¼ºåˆ¶åˆ†å‰²
                para_chunks = self._force_split_paragraph(paragraph)
                chunks.extend(para_chunks)
                continue
            
            # å¦‚æœæ·»åŠ è¿™ä¸ªæ®µè½ä¼šè¶…è¿‡æœ€å¤§é•¿åº¦
            potential_length = len(current_chunk + "\n\n" + paragraph) if current_chunk else len(paragraph)
            if potential_length > CHUNK_CONFIG['max_size']:
                # å¦‚æœå½“å‰å—å·²è¾¾åˆ°æœ€å°é•¿åº¦ï¼Œä¿å­˜å®ƒ
                if len(current_chunk) >= CHUNK_CONFIG['min_size']:
                    chunks.append(current_chunk.strip())
                    # æ·»åŠ é‡å éƒ¨åˆ†
                    overlap_text = current_chunk[-CHUNK_CONFIG['overlap']:] if CHUNK_CONFIG['overlap'] > 0 else ""
                    current_chunk = overlap_text + "\n\n" + paragraph
                else:
                    # å½“å‰å—å¤ªå°ï¼Œç»§ç»­æ·»åŠ 
                    current_chunk += "\n\n" + paragraph if current_chunk else paragraph
            else:
                # å¯ä»¥å®‰å…¨æ·»åŠ è¿™ä¸ªæ®µè½
                if current_chunk:
                    current_chunk += "\n\n" + paragraph
                else:
                    current_chunk = paragraph
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk.strip():
            chunks.append(current_chunk.strip())
        
        return chunks if chunks else [text]
    
    def _force_split_paragraph(self, paragraph: str) -> List[str]:
        """å¼ºåˆ¶åˆ†å‰²è¶…é•¿æ®µè½"""
        chunks = []
        max_size = CHUNK_CONFIG['max_size']
        overlap = CHUNK_CONFIG['overlap']
        
        start = 0
        while start < len(paragraph):
            # è®¡ç®—å½“å‰å—çš„ç»“æŸä½ç½®
            end = start + max_size
            
            if end >= len(paragraph):
                # æœ€åä¸€å—
                chunks.append(paragraph[start:].strip())
                break
            
            # å°è¯•åœ¨å¥å·ã€æ„Ÿå¹å·ã€é—®å·å¤„åˆ†å‰²
            best_split = end
            for i in range(end - 100, end + 1):
                if i < len(paragraph) and paragraph[i] in 'ã€‚ï¼ï¼Ÿ\n':
                    best_split = i + 1
                    break
            
            chunks.append(paragraph[start:best_split].strip())
            
            # ä¸‹ä¸€å—çš„å¼€å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            start = max(best_split - overlap, start + 1)
        
        return [chunk for chunk in chunks if chunk.strip()]
    
    def _load_or_create_index(self):
        """åŠ è½½å·²æœ‰ç´¢å¼•æˆ–åˆ›å»ºæ–°ç´¢å¼• - ä½¿ç”¨ä¸´æ—¶è‹±æ–‡è·¯å¾„è§£å†³ä¸­æ–‡è·¯å¾„é—®é¢˜"""
        import tempfile
        import shutil
        
        try:
            if self.index_file.exists() and self.mapping_file.exists():
                # ä½¿ç”¨ä¸´æ—¶è‹±æ–‡è·¯å¾„åŠ è½½ç´¢å¼•ï¼Œé¿å…FAISSä¸­æ–‡è·¯å¾„é—®é¢˜
                temp_dir = Path(tempfile.gettempdir()) / "zhishui_load"
                temp_dir.mkdir(exist_ok=True)
                temp_index_file = temp_dir / "faiss.index"
                temp_mapping_file = temp_dir / "chunk_mapping.json"
                
                try:
                    # å¤åˆ¶æ–‡ä»¶åˆ°ä¸´æ—¶è‹±æ–‡è·¯å¾„
                    shutil.copy2(str(self.index_file), str(temp_index_file))
                    shutil.copy2(str(self.mapping_file), str(temp_mapping_file))
                    
                    # ä»ä¸´æ—¶è·¯å¾„åŠ è½½ç´¢å¼•
                    self.faiss_index = faiss.read_index(str(temp_index_file))
                    with open(temp_mapping_file, 'r', encoding='utf-8') as f:
                        chunk_mapping = json.load(f)
                        # å…¼å®¹æ–°æ—§æ˜ å°„æ ¼å¼
                        if isinstance(chunk_mapping, list):
                            # å¤„ç†åˆ—è¡¨æ ¼å¼ï¼Œå¯èƒ½åŒ…å«å­—å…¸å¯¹è±¡
                            self.chunk_ids = []
                            for item in chunk_mapping:
                                if isinstance(item, dict) and 'chunk_id' in item:
                                    self.chunk_ids.append(item['chunk_id'])
                                else:
                                    self.chunk_ids.append(str(item))
                        elif isinstance(chunk_mapping, dict):
                            # å­—å…¸æ ¼å¼ï¼š{"0": "chunk_id_1", "1": "chunk_id_2", ...}
                            # æˆ–è€… {"0": {"chunk_id": "...", ...}, ...}
                            self.chunk_ids = []
                            for i in range(len(chunk_mapping)):
                                mapping_data = chunk_mapping.get(str(i), "")
                                if isinstance(mapping_data, dict) and 'chunk_id' in mapping_data:
                                    self.chunk_ids.append(mapping_data['chunk_id'])
                                else:
                                    self.chunk_ids.append(str(mapping_data))
                        else:
                            raise ValueError(f"ä¸æ”¯æŒçš„æ˜ å°„æ ¼å¼: {type(chunk_mapping)}")
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    temp_index_file.unlink()
                    temp_mapping_file.unlink()
                    temp_dir.rmdir()
                    
                    logger.info(f"å·²åŠ è½½ç°æœ‰å‘é‡ç´¢å¼•ï¼ŒåŒ…å«{len(self.chunk_ids)}ä¸ªå‘é‡")
                    
                except Exception as temp_error:
                    logger.error(f"ä¸´æ—¶è·¯å¾„åŠ è½½å¤±è´¥: {temp_error}")
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    if temp_index_file.exists():
                        temp_index_file.unlink()
                    if temp_mapping_file.exists():
                        temp_mapping_file.unlink()
                    if temp_dir.exists():
                        temp_dir.rmdir()
                    raise temp_error
                    
            else:
                # åˆ›å»ºæ–°ç´¢å¼• - ä½¿ç”¨L2è·ç¦»ç´¢å¼•ä¸é‡å»ºè„šæœ¬ä¿æŒä¸€è‡´
                self.faiss_index = faiss.IndexFlatL2(OLLAMA_CONFIG['embedding_dim'])
                self.chunk_ids = []
                logger.info("åˆ›å»ºäº†æ–°çš„å‘é‡ç´¢å¼•")
                
        except Exception as e:
            logger.error(f"ç´¢å¼•åŠ è½½å¤±è´¥: {e}")
            # åˆ›å»ºæ–°ç´¢å¼• - ä½¿ç”¨L2è·ç¦»ç´¢å¼•ä¸é‡å»ºè„šæœ¬ä¿æŒä¸€è‡´
            self.faiss_index = faiss.IndexFlatL2(OLLAMA_CONFIG['embedding_dim'])
            self.chunk_ids = []
    
    def add_document(self, doc_id: str, title: str, content: str) -> int:
        """æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•"""
        try:
            # æ™ºèƒ½åˆ†å—
            chunks = self._chunk_text(content)
            added_chunks = 0
            
            with self.db_manager.get_connection() as conn:
                for i, chunk in enumerate(chunks):
                    if not chunk.strip():
                        continue
                    
                    # ç”Ÿæˆå‘é‡
                    embedding = self._get_embedding(f"{title}\n{chunk}")
                    embedding = embedding / np.linalg.norm(embedding)  # å½’ä¸€åŒ–
                    
                    # æ·»åŠ åˆ°FAISSç´¢å¼•
                    self.faiss_index.add(embedding.reshape(1, -1))
                    
                    # åˆ›å»ºåˆ†å—ID
                    chunk_id = f"{doc_id}_chunk_{i}"
                    self.chunk_ids.append(chunk_id)
                    
                    # ä¿å­˜åˆ°æ•°æ®åº“
                    conn.execute("""
                        INSERT INTO document_chunks 
                        (chunk_id, doc_id, chunk_index, content, vector_id, char_count)
                        VALUES (?, ?, ?, ?, ?, ?)
                    """, (chunk_id, doc_id, i, chunk, len(self.chunk_ids)-1, len(chunk)))
                    
                    added_chunks += 1
                
                conn.commit()
            
            # ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶
            self._save_index()
            
            logger.info(f"æ–‡æ¡£{doc_id}æ·»åŠ åˆ°ç´¢å¼•ï¼Œå…±{added_chunks}ä¸ªåˆ†å—")
            return added_chunks
            
        except Exception as e:
            logger.error(f"æ·»åŠ æ–‡æ¡£åˆ°ç´¢å¼•å¤±è´¥: {e}")
            raise
    
    def search(self, query: str, top_k: int = 5, category: str = "") -> List[Dict]:
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        try:
            if not self.faiss_index or len(self.chunk_ids) == 0:
                return []
            
            # è·å–æŸ¥è¯¢å‘é‡
            query_embedding = self._get_embedding(query)
            query_embedding = query_embedding / np.linalg.norm(query_embedding)
            
            # FAISSæœç´¢
            scores, indices = self.faiss_index.search(
                query_embedding.reshape(1, -1), 
                min(top_k * 2, len(self.chunk_ids))  # æœç´¢æ›´å¤šç»“æœä»¥ä¾¿è¿‡æ»¤
            )
            
            # è·å–è¯¦ç»†ä¿¡æ¯
            results = []
            seen_docs = set()
            
            with self.db_manager.get_connection() as conn:
                for score, idx in zip(scores[0], indices[0]):
                    if idx < len(self.chunk_ids):
                        chunk_id = self.chunk_ids[idx]
                        
                        # è·å–æ–‡æ¡£ä¿¡æ¯
                        cursor = conn.execute("""
                            SELECT d.doc_id, d.original_filename, d.title, d.category, d.subcategory,
                                   c.content, c.chunk_index
                            FROM documents d
                            JOIN document_chunks c ON d.doc_id = c.doc_id
                            WHERE c.chunk_id = ? AND d.status = 'active'
                        """, (chunk_id,))
                        
                        row = cursor.fetchone()
                        if row:
                            doc_id = row['doc_id']
                            
                            # é¿å…é‡å¤æ–‡æ¡£
                            if doc_id in seen_docs:
                                continue
                            
                            # åˆ†ç±»è¿‡æ»¤
                            if category and row['category'] != category:
                                continue
                            
                            # å¤„ç†content_preview
                            content = row['content']
                            try:
                                # ç¡®ä¿contentæ˜¯å­—ç¬¦ä¸²ç±»å‹
                                if content:
                                    if isinstance(content, bytes):
                                        content = content.decode('utf-8', errors='replace')
                                    elif not isinstance(content, str):
                                        content = str(content)
                                    
                                    # ç›´æ¥æˆªå–å†…å®¹ï¼Œä¸è¿›è¡Œé¢å¤–çš„ç¼–ç æ“ä½œ
                                    content_preview = content[:200] + "..." if len(content) > 200 else content
                                    
                                    # ç¡®ä¿é¢„è§ˆå†…å®¹æ˜¯æœ‰æ•ˆçš„UTF-8å­—ç¬¦ä¸²
                                    content_preview = content_preview.encode('utf-8', errors='replace').decode('utf-8')
                                else:
                                    content_preview = "å†…å®¹é¢„è§ˆæš‚ä¸å¯ç”¨"
                            except Exception as e:
                                logger.warning(f"å¤„ç†content_previewæ—¶å‡ºé”™: {e}")
                                content_preview = "å†…å®¹é¢„è§ˆæš‚ä¸å¯ç”¨"
                            
                            result = {
                                "doc_id": row['doc_id'],
                                "title": row['title'],
                                "filename": row['original_filename'],
                                "category": row['category'],
                                "subcategory": row['subcategory'],
                                "content_preview": content_preview,
                                "similarity_score": float(score),
                                "chunk_index": row['chunk_index']
                            }
                            results.append(result)
                            seen_docs.add(doc_id)
                            
                            if len(results) >= top_k:
                                break
            
            return results
            
        except Exception as e:
            logger.error(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def _save_index(self):
        """ä¿å­˜ç´¢å¼•åˆ°æ–‡ä»¶"""
        try:
            logger.info(f"å¼€å§‹ä¿å­˜å‘é‡ç´¢å¼•åˆ°: {self.index_file}")
            logger.info(f"å½“å‰ç´¢å¼•åŒ…å« {len(self.chunk_ids)} ä¸ªå‘é‡")
            
            # æ£€æŸ¥ç´¢å¼•æ˜¯å¦æœ‰æ•ˆ
            if self.faiss_index is None:
                logger.error("FAISSç´¢å¼•ä¸ºç©ºï¼Œæ— æ³•ä¿å­˜")
                return
                
            if len(self.chunk_ids) == 0:
                logger.warning("æ²¡æœ‰åˆ†å—æ•°æ®ï¼Œè·³è¿‡ä¿å­˜")
                return
            
            # ç¡®ä¿ç›®å½•å­˜åœ¨
            self.index_file.parent.mkdir(parents=True, exist_ok=True)
            
            # æ£€æŸ¥ç›®å½•æƒé™
            if not os.access(str(self.index_file.parent), os.W_OK):
                logger.error(f"æ²¡æœ‰å†™å…¥æƒé™: {self.index_file.parent}")
                return
            
            # ä¿å­˜FAISSç´¢å¼•
            logger.info(f"æ­£åœ¨ä¿å­˜FAISSç´¢å¼•åˆ°: {self.index_file}")
            faiss.write_index(self.faiss_index, str(self.index_file))
            
            # éªŒè¯æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
            if self.index_file.exists():
                file_size = self.index_file.stat().st_size
                logger.info(f"FAISSç´¢å¼•å·²ä¿å­˜ï¼Œæ–‡ä»¶å¤§å°: {file_size} å­—èŠ‚")
            else:
                logger.error(f"FAISSç´¢å¼•æ–‡ä»¶æœªåˆ›å»º: {self.index_file}")
                return
            
            # ä¿å­˜chunk_idsæ˜ å°„
            logger.info(f"æ­£åœ¨ä¿å­˜åˆ†å—æ˜ å°„åˆ°: {self.mapping_file}")
            with open(self.mapping_file, 'w', encoding='utf-8') as f:
                json.dump(self.chunk_ids, f, ensure_ascii=False, indent=2)
            
            # éªŒè¯æ˜ å°„æ–‡ä»¶æ˜¯å¦åˆ›å»ºæˆåŠŸ
            if self.mapping_file.exists():
                logger.info(f"åˆ†å—æ˜ å°„å·²ä¿å­˜ï¼Œå…± {len(self.chunk_ids)} ä¸ªåˆ†å—")
            else:
                logger.error(f"åˆ†å—æ˜ å°„æ–‡ä»¶æœªåˆ›å»º: {self.mapping_file}")
                return
            
            logger.info(f"å‘é‡ç´¢å¼•ä¿å­˜æˆåŠŸ: {self.index_file}")
            logger.info(f"æ˜ å°„æ–‡ä»¶ä¿å­˜æˆåŠŸ: {self.mapping_file}")
        except Exception as e:
            logger.error(f"ä¿å­˜ç´¢å¼•å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            raise
    
    def remove_document(self, doc_id: str):
        """ä»ç´¢å¼•ä¸­ç§»é™¤æ–‡æ¡£ï¼ˆæ³¨æ„ï¼šFAISSä¸æ”¯æŒç›´æ¥åˆ é™¤ï¼Œéœ€è¦é‡å»ºç´¢å¼•ï¼‰"""
        logger.warning(f"æ–‡æ¡£{doc_id}å·²ä»æ•°æ®åº“åˆ é™¤ï¼Œå‘é‡ç´¢å¼•å°†åœ¨ä¸‹æ¬¡é‡å»ºæ—¶æ›´æ–°")

# ================================
# å…¨å±€å®ä¾‹
# ================================
db_manager = None
vector_engine = None

def init_service():
    """åˆå§‹åŒ–æœåŠ¡ç»„ä»¶"""
    global db_manager, vector_engine
    
    if db_manager is None:
        db_manager = DatabaseManager()
        logger.info("æ•°æ®åº“ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆ")
    
    if vector_engine is None:
        vector_engine = VectorSearchEngine(db_manager)
        logger.info("å‘é‡æ£€ç´¢å¼•æ“åˆå§‹åŒ–å®Œæˆ")
    
    # è‡ªåŠ¨æ‰«æå’Œå¯¼å…¥PDFæ–‡æ¡£
    auto_import_documents()

def auto_import_documents():
    """è‡ªåŠ¨æ‰«ædocumentsç›®å½•å¹¶å¯¼å…¥PDFæ–‡æ¡£"""
    try:
        # è·å–å·²å­˜åœ¨çš„æ–‡æ¡£
        existing_docs = []
        try:
            conn = db_manager.get_connection()
            cursor = conn.cursor()
            cursor.execute("SELECT original_filename FROM documents")
            existing_docs = [row[0] for row in cursor.fetchall()]
            conn.close()
        except Exception as e:
            logger.warning(f"è·å–å·²å­˜åœ¨æ–‡æ¡£å¤±è´¥: {e}")
        
        # æ‰«æPDFæ–‡ä»¶
        pdf_files = list(DOCUMENTS_DIR.glob("*.pdf"))
        new_imports = 0
        
        for pdf_file in pdf_files:
            filename = pdf_file.name
            
            # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
            if filename in existing_docs:
                continue
            
            try:
                # æå–PDFå†…å®¹
                with open(pdf_file, 'rb') as f:
                    pdf_reader = PyPDF2.PdfReader(f)
                    text_content = ""
                    page_count = len(pdf_reader.pages)
                    
                    for page in pdf_reader.pages:
                        text_content += page.extract_text() + "\n"
                    
                    file_size = pdf_file.stat().st_size
                
                if not text_content.strip():
                    logger.warning(f"PDFæ–‡ä»¶æ— æ–‡æœ¬å†…å®¹: {filename}")
                    continue
                
                # æ™ºèƒ½åˆ†ç±»
                filename_lower = filename.lower()
                if any(keyword in filename_lower for keyword in 
                       ['å˜ç”µç«™', 'ç”µç«™', 'å‘ç”µ', 'é…ç”µ', 'ç”µåŠ›', 'ç”µç½‘', 'æœºç»„']):
                    category = "ç”µåŠ›ç³»ç»Ÿè¿ç»´"
                elif any(keyword in filename_lower for keyword in 
                         ['æ°´åˆ©', 'æ°´ç”µ', 'å¤§å', 'æ°´åº“', 'çŒåŒº', 'æ°´æ–‡', 'é—¸é—¨', 'æ³µç«™']):
                    category = "æ°´åˆ©ç³»ç»Ÿè¿ç»´"
                elif any(keyword in filename_lower for keyword in 
                         ['é€šä¿¡', 'ç½‘ç»œ', 'æ•°æ®ä¼ è¾“', 'æ— çº¿', 'å…‰çº¤', 'ç½‘ç»œå®‰å…¨']):
                    category = "é€šä¿¡ç½‘ç»œè¿ç»´"
                elif any(keyword in filename_lower for keyword in 
                         ['ç›‘æ§', 'å®‰é˜²', 'é—¨ç¦', 'æŠ¥è­¦', 'å‘¨ç•Œ', 'æ¶ˆé˜²']):
                    category = "å®‰é˜²ç³»ç»Ÿè¿ç»´"
                else:
                    category = "è¿ç»´æ ‡å‡†è§„èŒƒ"
                
                # ä¿å­˜æ–‡æ¡£ä¿¡æ¯
                doc_id = str(uuid.uuid4())
                title = filename.replace('.pdf', '')
                
                conn = db_manager.get_connection()
                cursor = conn.cursor()
                cursor.execute("""
                    INSERT INTO documents 
                    (doc_id, original_filename, title, category, subcategory, file_path, 
                     upload_time, page_count, file_size)
                    VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                """, (
                    doc_id, filename, title, category, '', str(pdf_file),
                    datetime.now().isoformat(), page_count, file_size
                ))
                conn.commit()
                conn.close()
                
                # æ·»åŠ åˆ°å‘é‡ç´¢å¼•
                if vector_engine:
                    vector_engine.add_document(doc_id, title, text_content)
                
                new_imports += 1
                logger.info(f"è‡ªåŠ¨å¯¼å…¥æ–‡æ¡£: {filename} ({category})")
                
            except Exception as e:
                logger.error(f"è‡ªåŠ¨å¯¼å…¥æ–‡æ¡£å¤±è´¥ {filename}: {e}")
        
        if new_imports > 0:
            logger.info(f"è‡ªåŠ¨å¯¼å…¥å®Œæˆï¼Œæ–°å¢ {new_imports} ä¸ªæ–‡æ¡£")
        else:
            logger.info("æœªå‘ç°æ–°çš„PDFæ–‡æ¡£")
            
    except Exception as e:
        logger.error(f"è‡ªåŠ¨å¯¼å…¥è¿‡ç¨‹å‡ºé”™: {e}")

# ================================
# MCPå·¥å…·å‡½æ•°
# ================================

@mcp.tool()
def import_file_to_knowledge(
    file_path: str,
    title: str,
    category: str,
    subcategory: str = ""
) -> str:
    """
    å¯¼å…¥æ–‡ä»¶åˆ°è¿ç»´çŸ¥è¯†åº“
    
    ä¸“ä¸ºå››å·æ™ºæ°´ä¿¡æ¯æŠ€æœ¯æœ‰é™å…¬å¸è¿ç»´çŸ¥è¯†ç®¡ç†è®¾è®¡
    æ”¯æŒç”µåŠ›ã€æ°´åˆ©ã€é€šä¿¡ã€å®‰é˜²ç­‰ä¸“ä¸šé¢†åŸŸæ–‡æ¡£
    æ”¯æŒPDFå’ŒTXTæ ¼å¼æ–‡ä»¶
    
    Args:
        file_path (str): æœ¬åœ°æ–‡ä»¶è·¯å¾„ï¼ˆPDFæˆ–TXTæ–‡ä»¶ï¼‰
        title (str): æ–‡æ¡£æ ‡é¢˜
        category (str): æ–‡æ¡£åˆ†ç±»ï¼ˆç”µåŠ›ç³»ç»Ÿè¿ç»´/æ°´åˆ©ç³»ç»Ÿè¿ç»´/é€šä¿¡ç½‘ç»œè¿ç»´/å®‰é˜²ç³»ç»Ÿè¿ç»´/è¿ç»´æ ‡å‡†è§„èŒƒï¼‰
        subcategory (str): å­åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        str: å¯¼å…¥ç»“æœ
    """
    try:
        # åˆå§‹åŒ–æœåŠ¡
        init_service()
        
        # å‚æ•°éªŒè¯
        if not all([file_path, title, category]):
            return json.dumps({"error": "å¿…å¡«å‚æ•°ä¸èƒ½ä¸ºç©º"}, ensure_ascii=False)
        
        if category not in KNOWLEDGE_CATEGORIES:
            return json.dumps({
                "error": f"æ— æ•ˆçš„åˆ†ç±»ï¼Œæ”¯æŒçš„åˆ†ç±»: {list(KNOWLEDGE_CATEGORIES.keys())}"
            }, ensure_ascii=False)
        
        # éªŒè¯æ–‡ä»¶è·¯å¾„
        source_file = Path(file_path)
        if not source_file.exists():
            return json.dumps({"error": f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}"}, ensure_ascii=False)
        
        if not source_file.is_file():
            return json.dumps({"error": f"è·¯å¾„ä¸æ˜¯æ–‡ä»¶: {file_path}"}, ensure_ascii=False)
        
        # è·å–æ–‡ä»¶åå’Œæ‰©å±•å
        filename = source_file.name
        file_size = source_file.stat().st_size
        
        # ç”Ÿæˆæ–‡æ¡£IDå’Œç›®æ ‡æ–‡ä»¶è·¯å¾„
        doc_id = str(uuid.uuid4())
        
        # æ ¹æ®æ–‡ä»¶æ‰©å±•åç¡®å®šå¤„ç†æ–¹å¼
        file_ext = filename.lower().split('.')[-1] if '.' in filename else 'pdf'
        logger.info(f"æ–‡ä»¶å: {filename}, æ£€æµ‹åˆ°æ‰©å±•å: {file_ext}")
        
        if file_ext == 'txt':
            # å¤„ç†TXTæ–‡ä»¶
            target_file_path = DOCUMENTS_DIR / f"{doc_id}.txt"
            
            # å¤åˆ¶TXTæ–‡ä»¶åˆ°ç›®æ ‡ä½ç½®
            import shutil
            shutil.copy2(source_file, target_file_path)
            
            # æå–TXTæ–‡æœ¬
            try:
                with open(target_file_path, 'r', encoding='utf-8') as f:
                    text_content = f.read()
                
                if not text_content.strip():
                    return json.dumps({"error": "TXTæ–‡ä»¶å†…å®¹ä¸ºç©º"}, ensure_ascii=False)
                
                page_count = 1  # TXTæ–‡ä»¶è§†ä¸º1é¡µ
                    
            except Exception as e:
                # æ¸…ç†å·²ä¿å­˜çš„æ–‡ä»¶
                if target_file_path.exists():
                    target_file_path.unlink()
                return json.dumps({"error": f"TXTæ–‡æœ¬è¯»å–å¤±è´¥: {str(e)}"}, ensure_ascii=False)
        
        else:
            # å¤„ç†PDFæ–‡ä»¶
            target_file_path = DOCUMENTS_DIR / f"{doc_id}.pdf"
            
            # å¤åˆ¶PDFæ–‡ä»¶åˆ°ç›®æ ‡ä½ç½®
            import shutil
            shutil.copy2(source_file, target_file_path)
            
            # æå–PDFæ–‡æœ¬ - ä½¿ç”¨å¤šç§æ–¹æ³•ç¡®ä¿æœ€ä½³å…¼å®¹æ€§
            text_content = ""
            page_count = 0
            extraction_success = False
            
            # æ–¹æ³•1: ä½¿ç”¨pdfplumberï¼ˆå¯¹ä¸­æ–‡æ”¯æŒæœ€å¥½ï¼‰
            try:
                with pdfplumber.open(target_file_path) as pdf:
                    page_count = len(pdf.pages)
                    logger.info(f"ä½¿ç”¨pdfplumberå¤„ç†PDFï¼Œå…±{page_count}é¡µ")
                    
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text_content += page_text + "\n\n"  # ä½¿ç”¨åŒæ¢è¡Œç¬¦ç¡®ä¿æ®µè½åˆ†éš”
                    
                    if text_content.strip():
                        extraction_success = True
                        logger.info(f"pdfplumberæå–æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(text_content)}")
                        
            except Exception as e:
                logger.warning(f"pdfplumberæå–å¤±è´¥: {e}")
            
            # æ–¹æ³•2: å¦‚æœpdfplumberå¤±è´¥ï¼Œä½¿ç”¨pymupdfä½œä¸ºå¤‡é€‰
            if not extraction_success:
                try:
                    doc = fitz.open(target_file_path)
                    page_count = doc.page_count
                    logger.info(f"ä½¿ç”¨pymupdfå¤„ç†PDFï¼Œå…±{page_count}é¡µ")
                    
                    for page_num in range(page_count):
                        page = doc[page_num]
                        page_text = page.get_text()
                        if page_text:
                            text_content += page_text + "\n\n"  # ä½¿ç”¨åŒæ¢è¡Œç¬¦ç¡®ä¿æ®µè½åˆ†éš”
                    
                    doc.close()
                    
                    if text_content.strip():
                        extraction_success = True
                        logger.info(f"pymupdfæå–æˆåŠŸï¼Œæ–‡æœ¬é•¿åº¦: {len(text_content)}")
                        
                except Exception as e:
                    logger.warning(f"pymupdfæå–å¤±è´¥: {e}")
            
            # æ£€æŸ¥æå–ç»“æœ
            if not extraction_success or not text_content.strip():
                # æ¸…ç†å·²ä¿å­˜çš„æ–‡ä»¶
                if target_file_path.exists():
                    target_file_path.unlink()
                return json.dumps({
                    "error": "PDFæ–‡ä»¶æ— æ³•æå–æ–‡æœ¬å†…å®¹ï¼Œå¯èƒ½æ˜¯æ‰«æç‰ˆPDFæˆ–æ–‡ä»¶æŸåã€‚å»ºè®®ä½¿ç”¨OCRå·¥å…·å¤„ç†æ‰«æç‰ˆPDFã€‚"
                }, ensure_ascii=False)
        
        # ä¿å­˜æ–‡æ¡£å…ƒæ•°æ®
        with db_manager.get_connection() as conn:
            conn.execute("""
                INSERT INTO documents 
                (doc_id, original_filename, title, category, subcategory, file_path, 
                 upload_time, page_count, file_size)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                doc_id, filename, title, category, subcategory or "",
                str(target_file_path), datetime.now().isoformat(),
                page_count, file_size
            ))
            conn.commit()
        
        # æ·»åŠ åˆ°å‘é‡ç´¢å¼•
        logger.info(f"å¼€å§‹æ·»åŠ æ–‡æ¡£åˆ°å‘é‡ç´¢å¼•: {title}")
        logger.info(f"æ–‡æœ¬å†…å®¹é•¿åº¦: {len(text_content)} å­—ç¬¦")
        try:
            chunk_count = vector_engine.add_document(doc_id, title, text_content)
            logger.info(f"å‘é‡ç´¢å¼•æ·»åŠ æˆåŠŸï¼Œåˆ›å»ºäº† {chunk_count} ä¸ªåˆ†å—")
        except Exception as e:
            logger.error(f"å‘é‡ç´¢å¼•æ·»åŠ å¤±è´¥: {e}")
            import traceback
            logger.error(f"è¯¦ç»†é”™è¯¯ä¿¡æ¯: {traceback.format_exc()}")
            # å³ä½¿å‘é‡ç´¢å¼•å¤±è´¥ï¼Œä¹Ÿè¿”å›æˆåŠŸï¼Œä½†æ ‡è®°chunk_countä¸º0
            chunk_count = 0
        
        result = {
            "success": True,
            "doc_id": doc_id,
            "filename": filename,
            "title": title,
            "category": category,
            "subcategory": subcategory,
            "page_count": page_count,
            "file_size_mb": round(file_size / 1024 / 1024, 2),
            "chunks_created": chunk_count,
            "upload_time": datetime.now().isoformat(),
            "message": f"æ–‡æ¡£å·²æˆåŠŸå¯¼å…¥åˆ°{COMPANY_NAME}è¿ç»´çŸ¥è¯†åº“"
        }
        
        logger.info(f"æ–‡æ¡£å¯¼å…¥æˆåŠŸ: {title} ({category})")
        return json.dumps(result, ensure_ascii=False, indent=2)
        
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¯¼å…¥å¤±è´¥: {e}")
        return json.dumps({"error": f"å¯¼å…¥å¤±è´¥: {str(e)}"}, ensure_ascii=False)

@mcp.tool()
def search_knowledge(
    query: str,
    top_k: int = 5,
    category: str = ""
) -> str:
    """
    æœç´¢è¿ç»´çŸ¥è¯†åº“
    
    åŸºäºRAGæŠ€æœ¯çš„æ™ºèƒ½çŸ¥è¯†æ£€ç´¢ï¼Œä½¿ç”¨qwen3-embeddingæ¨¡å‹
    ä¸“ä¸ºå››å·æ™ºæ°´è¿ç»´å›¢é˜Ÿæä¾›ç²¾å‡†çš„æŠ€æœ¯çŸ¥è¯†æŸ¥è¯¢
    
    Args:
        query (str): æœç´¢æŸ¥è¯¢è¯­å¥
        top_k (int): è¿”å›ç»“æœæ•°é‡ï¼ˆé»˜è®¤5ï¼‰
        category (str): é™å®šæœç´¢åˆ†ç±»ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        str: æœç´¢ç»“æœ
    """
    try:
        # åˆå§‹åŒ–æœåŠ¡
        init_service()
        
        # å‚æ•°éªŒè¯
        if not query or not query.strip():
            return json.dumps({"error": "æŸ¥è¯¢å†…å®¹ä¸èƒ½ä¸ºç©º"}, ensure_ascii=False)
        
        if category and category not in KNOWLEDGE_CATEGORIES:
            return json.dumps({
                "error": f"æ— æ•ˆçš„åˆ†ç±»ï¼Œæ”¯æŒçš„åˆ†ç±»: {list(KNOWLEDGE_CATEGORIES.keys())}"
            }, ensure_ascii=False)
        
        # æ‰§è¡Œæœç´¢
        results = vector_engine.search(query, top_k, category)
        
        # æ ¼å¼åŒ–ç»“æœ
        response = {
            "query": query,
            "category_filter": category or "å…¨éƒ¨åˆ†ç±»",
            "total_found": len(results),
            "search_time": datetime.now().isoformat(),
            "knowledge_source": f"{COMPANY_NAME}è¿ç»´çŸ¥è¯†åº“",
            "results": []
        }
        
        for i, result in enumerate(results, 1):
            formatted_result = {
                "rank": i,
                "title": result['title'],
                "filename": result['filename'],
                "category": result['category'],
                "subcategory": result['subcategory'],
                "similarity_score": round(result['similarity_score'], 4),
                "content_preview": result['content_preview'],
                "doc_id": result['doc_id']
            }
            response["results"].append(formatted_result)
        
        if not results:
            response["message"] = "æœªæ‰¾åˆ°ç›¸å…³æ–‡æ¡£ï¼Œå»ºè®®ï¼š\n1. å°è¯•æ›´æ¢å…³é”®è¯\n2. å‡å°‘æœç´¢æ¡ä»¶\n3. æ£€æŸ¥åˆ†ç±»ç­›é€‰"
        
        logger.info(f"çŸ¥è¯†æœç´¢å®Œæˆ: {query} - æ‰¾åˆ°{len(results)}ä¸ªç»“æœ")
        
        # ç¡®ä¿ä¸­æ–‡ç¼–ç æ­£ç¡®å¤„ç†
        json_str = json.dumps(response, ensure_ascii=False, indent=2)
        # ç¡®ä¿è¿”å›çš„æ˜¯æ­£ç¡®ç¼–ç çš„å­—ç¬¦ä¸²
        if isinstance(json_str, bytes):
            json_str = json_str.decode('utf-8')
        
        # è°ƒè¯•æ—¥å¿—ï¼šè®°å½•å®é™…è¿”å›çš„å†…å®¹
        if results:
            first_preview = response['results'][0]['content_preview'][:50]
            logger.info(f"è¿”å›å†…å®¹é¢„è§ˆ: {repr(first_preview)}")
        
        return json_str
        
    except Exception as e:
        logger.error(f"çŸ¥è¯†æœç´¢å¤±è´¥: {e}")
        return json.dumps({"error": f"æœç´¢å¤±è´¥: {str(e)}"}, ensure_ascii=False)

@mcp.tool()
def manage_documents(
    action: str,
    doc_id: str = "",
    category: str = ""
) -> str:
    """
    æ–‡æ¡£ç®¡ç†æ“ä½œ
    
    æ”¯æŒæŸ¥çœ‹ã€åˆ é™¤å’Œç»Ÿè®¡è¿ç»´çŸ¥è¯†åº“ä¸­çš„æ–‡æ¡£
    
    Args:
        action (str): æ“ä½œç±»å‹ï¼ˆlist/delete/info/statsï¼‰
        doc_id (str): æ–‡æ¡£IDï¼ˆåˆ é™¤å’ŒæŸ¥çœ‹è¯¦æƒ…æ—¶éœ€è¦ï¼‰
        category (str): åˆ†ç±»è¿‡æ»¤ï¼ˆåˆ—è¡¨æ—¶å¯é€‰ï¼‰
        
    Returns:
        str: æ“ä½œç»“æœ
    """
    try:
        # åˆå§‹åŒ–æœåŠ¡
        init_service()
        
        # è°ƒè¯•ä¿¡æ¯ï¼šæ˜¾ç¤ºæ•°æ®åº“è·¯å¾„
        import os
        current_dir = os.getcwd()
        db_path = os.path.join(current_dir, "knowledge_base.db")
        logger.info(f"MCPæœåŠ¡è°ƒè¯• - å·¥ä½œç›®å½•: {current_dir}")
        logger.info(f"MCPæœåŠ¡è°ƒè¯• - æ•°æ®åº“è·¯å¾„: {db_path}")
        logger.info(f"MCPæœåŠ¡è°ƒè¯• - æ•°æ®åº“æ–‡ä»¶å­˜åœ¨: {os.path.exists(db_path)}")
        
        # å‚æ•°éªŒè¯
        if not action:
            return json.dumps({"error": "æ“ä½œç±»å‹ä¸èƒ½ä¸ºç©º"}, ensure_ascii=False)
        
        if action == "list":
            return _list_documents(category)
        elif action == "delete":
            if not doc_id:
                return json.dumps({"error": "åˆ é™¤æ“ä½œéœ€è¦æä¾›doc_id"}, ensure_ascii=False)
            return _delete_document(doc_id)
        elif action == "info":
            if not doc_id:
                return json.dumps({"error": "æŸ¥çœ‹è¯¦æƒ…éœ€è¦æä¾›doc_id"}, ensure_ascii=False)
            return _get_document_info(doc_id)
        elif action == "stats":
            return _get_knowledge_stats()
        else:
            return json.dumps({
                "error": f"æ— æ•ˆçš„æ“ä½œç±»å‹: {action}",
                "supported_actions": ["list", "delete", "info", "stats"]
            }, ensure_ascii=False)
            
    except Exception as e:
        logger.error(f"æ–‡æ¡£ç®¡ç†æ“ä½œå¤±è´¥: {e}")
        return json.dumps({"error": f"æ“ä½œå¤±è´¥: {str(e)}"}, ensure_ascii=False)

def _list_documents(category: str = "") -> str:
    """åˆ—å‡ºæ–‡æ¡£"""
    with db_manager.get_connection() as conn:
        if category:
            cursor = conn.execute("""
                SELECT doc_id, title, original_filename, category, subcategory,
                       upload_time, page_count, file_size
                FROM documents 
                WHERE category = ? AND status = 'active'
                ORDER BY upload_time DESC
            """, (category,))
        else:
            cursor = conn.execute("""
                SELECT doc_id, title, original_filename, category, subcategory,
                       upload_time, page_count, file_size
                FROM documents 
                WHERE status = 'active'
                ORDER BY upload_time DESC
            """)
        
        documents = [dict(row) for row in cursor.fetchall()]
    
    response = {
        "action": "list_documents",
        "category_filter": category or "å…¨éƒ¨åˆ†ç±»",
        "total_documents": len(documents),
        "knowledge_base": f"{COMPANY_NAME}è¿ç»´çŸ¥è¯†åº“",
        "documents": []
    }
    
    for doc in documents:
        doc_info = {
            "doc_id": doc['doc_id'],
            "title": doc['title'],
            "filename": doc['original_filename'],
            "category": doc['category'],
            "subcategory": doc['subcategory'],
            "upload_time": doc['upload_time'],
            "page_count": doc['page_count'],
            "file_size_mb": round(doc['file_size'] / 1024 / 1024, 2) if doc['file_size'] else 0
        }
        response["documents"].append(doc_info)
    
    return json.dumps(response, ensure_ascii=False, indent=2)

def _delete_document(doc_id: str) -> str:
    """åˆ é™¤æ–‡æ¡£"""
    with db_manager.get_connection() as conn:
        # æ£€æŸ¥æ–‡æ¡£æ˜¯å¦å­˜åœ¨
        cursor = conn.execute("""
            SELECT title, file_path FROM documents WHERE doc_id = ? AND status = 'active'
        """, (doc_id,))
        doc = cursor.fetchone()
        
        if not doc:
            return json.dumps({"error": "æ–‡æ¡£ä¸å­˜åœ¨æˆ–å·²è¢«åˆ é™¤"}, ensure_ascii=False)
        
        # æ ‡è®°ä¸ºåˆ é™¤çŠ¶æ€
        conn.execute("""
            UPDATE documents SET status = 'deleted' WHERE doc_id = ?
        """, (doc_id,))
        
        # åˆ é™¤åˆ†å—æ•°æ®
        conn.execute("""
            DELETE FROM document_chunks WHERE doc_id = ?
        """, (doc_id,))
        
        conn.commit()
    
    # åˆ é™¤ç‰©ç†æ–‡ä»¶
    try:
        file_path = Path(doc['file_path'])
        if file_path.exists():
            file_path.unlink()
    except Exception as e:
        logger.warning(f"åˆ é™¤ç‰©ç†æ–‡ä»¶å¤±è´¥: {e}")
    
    # ä»å‘é‡ç´¢å¼•ä¸­ç§»é™¤
    vector_engine.remove_document(doc_id)
    
    result = {
        "success": True,
        "action": "delete_document",
        "doc_id": doc_id,
        "title": doc['title'],
        "message": "æ–‡æ¡£å·²æˆåŠŸåˆ é™¤",
        "note": "å‘é‡ç´¢å¼•å°†åœ¨ç³»ç»Ÿé‡å¯æ—¶è‡ªåŠ¨æ›´æ–°"
    }
    
    logger.info(f"æ–‡æ¡£åˆ é™¤æˆåŠŸ: {doc['title']}")
    return json.dumps(result, ensure_ascii=False, indent=2)

def _get_document_info(doc_id: str) -> str:
    """è·å–æ–‡æ¡£è¯¦ç»†ä¿¡æ¯"""
    with db_manager.get_connection() as conn:
        cursor = conn.execute("""
            SELECT * FROM documents WHERE doc_id = ? AND status = 'active'
        """, (doc_id,))
        doc = cursor.fetchone()
        
        if not doc:
            return json.dumps({"error": "æ–‡æ¡£ä¸å­˜åœ¨"}, ensure_ascii=False)
        
        # è·å–åˆ†å—ç»Ÿè®¡
        cursor = conn.execute("""
            SELECT COUNT(*) as chunk_count, AVG(char_count) as avg_chunk_size
            FROM document_chunks WHERE doc_id = ?
        """, (doc_id,))
        chunk_stats = cursor.fetchone()
    
    doc_info = {
        "doc_id": doc['doc_id'],
        "title": doc['title'],
        "filename": doc['original_filename'],
        "category": doc['category'],
        "subcategory": doc['subcategory'],
        "upload_time": doc['upload_time'],
        "page_count": doc['page_count'],
        "file_size_mb": round(doc['file_size'] / 1024 / 1024, 2) if doc['file_size'] else 0,
        "chunk_count": chunk_stats['chunk_count'],
        "avg_chunk_size": round(chunk_stats['avg_chunk_size']) if chunk_stats['avg_chunk_size'] else 0,
        "file_path": doc['file_path']
    }
    
    return json.dumps(doc_info, ensure_ascii=False, indent=2)

def _get_knowledge_stats() -> str:
    """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    with db_manager.get_connection() as conn:
        # æ€»ä½“ç»Ÿè®¡
        cursor = conn.execute("""
            SELECT COUNT(*) as total_docs, 
                   SUM(page_count) as total_pages,
                   SUM(file_size) as total_size
            FROM documents WHERE status = 'active'
        """)
        overall_stats = cursor.fetchone()
        
        # æŒ‰åˆ†ç±»ç»Ÿè®¡
        cursor = conn.execute("""
            SELECT category, COUNT(*) as doc_count, AVG(page_count) as avg_pages
            FROM documents WHERE status = 'active'
            GROUP BY category
            ORDER BY doc_count DESC
        """)
        category_stats = [dict(row) for row in cursor.fetchall()]
        
        # åˆ†å—ç»Ÿè®¡
        cursor = conn.execute("""
            SELECT COUNT(*) as total_chunks, AVG(char_count) as avg_chunk_size
            FROM document_chunks
        """)
        chunk_stats = cursor.fetchone()
    
    stats = {
        "knowledge_base": f"{COMPANY_NAME}è¿ç»´çŸ¥è¯†åº“",
        "statistics_time": datetime.now().isoformat(),
        "overall": {
            "total_documents": overall_stats['total_docs'],
            "total_pages": overall_stats['total_pages'],
            "total_size_mb": round(overall_stats['total_size'] / 1024 / 1024, 2) if overall_stats['total_size'] else 0,
            "total_chunks": chunk_stats['total_chunks'],
            "avg_chunk_size": round(chunk_stats['avg_chunk_size']) if chunk_stats['avg_chunk_size'] else 0
        },
        "by_category": [],
        "supported_categories": list(KNOWLEDGE_CATEGORIES.keys())
    }
    
    for cat_stat in category_stats:
        category_info = {
            "category": cat_stat['category'],
            "document_count": cat_stat['doc_count'],
            "avg_pages": round(cat_stat['avg_pages'], 1) if cat_stat['avg_pages'] else 0,
            "subcategories": KNOWLEDGE_CATEGORIES.get(cat_stat['category'], [])
        }
        stats["by_category"].append(category_info)
    
    return json.dumps(stats, ensure_ascii=False, indent=2)

# ================================
# æœåŠ¡å¯åŠ¨
# ================================

if __name__ == "__main__":
    logger.info(f"å¯åŠ¨ {COMPANY_NAME} è¿ç»´çŸ¥è¯†åº“MCPæœåŠ¡")
    
    # å°†å¯åŠ¨ä¿¡æ¯è®°å½•åˆ°æ—¥å¿—è€Œä¸æ˜¯stdoutï¼Œé¿å…å¹²æ‰°MCPé€šä¿¡
    logger.info("========================================")
    logger.info(f"  {COMPANY_NAME}")
    logger.info("  è¿ç»´çŸ¥è¯†åº“MCPæœåŠ¡")
    logger.info("========================================")
    logger.info("æœåŠ¡åŠŸèƒ½ï¼š")
    logger.info("[âˆš] PDFæ–‡æ¡£å¯¼å…¥å’Œæ™ºèƒ½åˆ†å—")
    logger.info("[âˆš] åŸºäºqwen3-embeddingçš„å‘é‡æ£€ç´¢")
    logger.info("[âˆš] äº”å¤§ä¸“ä¸šåˆ†ç±»ç®¡ç†")
    logger.info("[âˆš] é•¿æœŸæœ¬åœ°å­˜å‚¨")
    logger.info("ä¸“ä¸šåˆ†ç±»ï¼š")
    logger.info("- ç”µåŠ›ç³»ç»Ÿè¿ç»´ï¼ˆæ°´ç”µç«™ã€ç«ç”µç«™ã€æ–°èƒ½æºç­‰ï¼‰")
    logger.info("- æ°´åˆ©ç³»ç»Ÿè¿ç»´ï¼ˆå¤§åç›‘æµ‹ã€æ°´åº“ç®¡ç†ç­‰ï¼‰")
    logger.info("- é€šä¿¡ç½‘ç»œè¿ç»´ï¼ˆé€šä¿¡è®¾å¤‡ã€ç½‘ç»œç³»ç»Ÿç­‰ï¼‰")
    logger.info("- å®‰é˜²ç³»ç»Ÿè¿ç»´ï¼ˆç›‘æ§ã€é—¨ç¦ã€æŠ¥è­¦ç­‰ï¼‰")
    logger.info("- è¿ç»´æ ‡å‡†è§„èŒƒï¼ˆæµç¨‹ã€å®‰å…¨ã€æ ‡å‡†ç­‰ï¼‰")
    logger.info("æŠ€æœ¯ç‰¹æ€§ï¼š")
    logger.info("- RAGæ£€ç´¢ï¼šqwen3-embedding + FAISS")
    logger.info("- æœ¬åœ°å­˜å‚¨ï¼šSQLite + æ–‡ä»¶ç³»ç»Ÿ")
    logger.info("- æ™ºèƒ½åˆ†å—ï¼š500-1000å­—ç¬¦è‡ªé€‚åº”")
    logger.info("æ­£åœ¨å¯åŠ¨æœåŠ¡...")
    logger.info("========================================")
    
    try:
        # åˆå§‹åŒ–æœåŠ¡ç»„ä»¶
        init_service()
        logger.info("æ‰€æœ‰æœåŠ¡ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")
        
        # å¯åŠ¨MCPæœåŠ¡ - ä½¿ç”¨stdioä¼ è¾“ï¼ˆæ ‡å‡†è¾“å…¥è¾“å‡ºï¼‰
        mcp.run()
    except KeyboardInterrupt:
        logger.info("æ­£åœ¨å…³é—­æœåŠ¡...")
    except Exception as e:
        logger.error(f"æœåŠ¡å¯åŠ¨å¤±è´¥: {e}")
    finally:
        logger.info("æœåŠ¡å·²å…³é—­")

# ================================
# ä½¿ç”¨è¯´æ˜
# ================================
"""
ğŸš€ å››å·æ™ºæ°´è¿ç»´çŸ¥è¯†åº“MCPæœåŠ¡ä½¿ç”¨æŒ‡å—

ğŸ“‹ åŠŸèƒ½æ¦‚è¿°ï¼š
ä¸“ä¸ºå››å·æ™ºæ°´ä¿¡æ¯æŠ€æœ¯æœ‰é™å…¬å¸è®¾è®¡çš„è¿ç»´çŸ¥è¯†åº“ï¼Œ
æ”¯æŒç”µåŠ›ã€æ°´åˆ©è¡Œä¸šä¿¡æ¯åŒ–ç³»ç»Ÿçš„è¿ç»´çŸ¥è¯†ç®¡ç†ã€‚

ğŸ”§ å·¥å…·ä½¿ç”¨ï¼š

1. å¯¼å…¥æ–‡æ¡£ï¼š
   import_document(
       file_path="C:\\path\\to\\è¿ç»´æ‰‹å†Œ.pdf",
       title="æ°´ç”µç«™è¿ç»´æ ‡å‡†æ“ä½œæ‰‹å†Œ",
       category="ç”µåŠ›ç³»ç»Ÿè¿ç»´",
       subcategory="æ°´ç”µç«™"
   )

2. æœç´¢çŸ¥è¯†ï¼š
   search_knowledge(
       query="æ°´ç”µç«™æ•…éšœå¤„ç†æµç¨‹",
       top_k=5,
       category="ç”µåŠ›ç³»ç»Ÿè¿ç»´"
   )

3. ç®¡ç†æ–‡æ¡£ï¼š
   manage_documents(
       action="list",          # list/delete/info/stats
       category="æ°´åˆ©ç³»ç»Ÿè¿ç»´"
   )

ğŸ“‚ æ”¯æŒçš„åˆ†ç±»ï¼š
- ç”µåŠ›ç³»ç»Ÿè¿ç»´ï¼šæ°´ç”µç«™ã€ç«ç”µç«™ã€æ–°èƒ½æºç«™ã€å˜ç”µç«™ç­‰
- æ°´åˆ©ç³»ç»Ÿè¿ç»´ï¼šå¤§åç›‘æµ‹ã€æ°´åº“ç®¡ç†ã€çŒåŒºç³»ç»Ÿç­‰
- é€šä¿¡ç½‘ç»œè¿ç»´ï¼šé€šä¿¡è®¾å¤‡ã€ç½‘ç»œç³»ç»Ÿã€å…‰çº¤ç½‘ç»œç­‰  
- å®‰é˜²ç³»ç»Ÿè¿ç»´ï¼šè§†é¢‘ç›‘æ§ã€é—¨ç¦ç³»ç»Ÿã€æŠ¥è­¦ç³»ç»Ÿç­‰
- è¿ç»´æ ‡å‡†è§„èŒƒï¼šæ“ä½œæµç¨‹ã€å®‰å…¨è§„èŒƒã€æŠ€æœ¯æ ‡å‡†ç­‰

âš¡ æŠ€æœ¯ç‰¹ç‚¹ï¼š
- ä½¿ç”¨qwen3-embeddingæ¨¡å‹è¿›è¡Œå‘é‡åŒ–
- FAISSå‘é‡æ•°æ®åº“å®ç°å¿«é€Ÿæ£€ç´¢
- æ™ºèƒ½æ–‡æœ¬åˆ†å—ä¿æŒè¯­ä¹‰å®Œæ•´æ€§
- æœ¬åœ°å­˜å‚¨ç¡®ä¿æ•°æ®å®‰å…¨å’Œé•¿æœŸå¯ç”¨

ğŸ’¡ æœ€ä½³å®è·µï¼š
1. æ–‡æ¡£æ ‡é¢˜è¦æ¸…æ™°æè¿°å†…å®¹ä¸»é¢˜
2. é€‰æ‹©å‡†ç¡®çš„åˆ†ç±»å’Œå­åˆ†ç±»
3. æœç´¢æ—¶ä½¿ç”¨ä¸“ä¸šæœ¯è¯­è·å¾—æ›´å¥½ç»“æœ
4. å®šæœŸæŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯äº†è§£çŸ¥è¯†åº“çŠ¶å†µ
"""