#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤PDFæ–‡æ¡£ç¼–ç é—®é¢˜
è§£å†³PyPDF2æå–ä¸­æ–‡æ–‡æœ¬æ—¶çš„ç¼–ç ä¹±ç é—®é¢˜
"""

import os
import sys
import json
import sqlite3
import logging
from pathlib import Path
from typing import Dict, List

try:
    import PyPDF2
    import pdfplumber  # æ›´å¥½çš„PDFæ–‡æœ¬æå–åº“
except ImportError as e:
    print(f"ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·å®‰è£…: pip install PyPDF2 pdfplumber")
    sys.exit(1)

# é…ç½®è·¯å¾„
DATA_DIR = Path("knowledge_base")
DOCUMENTS_DIR = DATA_DIR / "documents"
DB_PATH = DATA_DIR / "metadata.db"
LOGS_DIR = Path("logs")

# ç¡®ä¿ç›®å½•å­˜åœ¨
for dir_path in [LOGS_DIR]:
    dir_path.mkdir(parents=True, exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOGS_DIR / 'fix_encoding.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

def extract_pdf_text_improved(pdf_path: str) -> str:
    """
    ä½¿ç”¨pdfplumberæ”¹è¿›PDFæ–‡æœ¬æå–
    
    Args:
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
        
    Returns:
        str: æå–çš„æ–‡æœ¬å†…å®¹
    """
    try:
        # é¦–å…ˆå°è¯•ä½¿ç”¨pdfplumberï¼ˆå¯¹ä¸­æ–‡æ”¯æŒæ›´å¥½ï¼‰
        with pdfplumber.open(pdf_path) as pdf:
            text_content = ""
            for page in pdf.pages:
                page_text = page.extract_text()
                if page_text:
                    text_content += page_text + "\n"
            
            if text_content.strip():
                logger.info(f"ä½¿ç”¨pdfplumberæˆåŠŸæå–æ–‡æœ¬: {len(text_content)}å­—ç¬¦")
                return text_content.strip()
    
    except Exception as e:
        logger.warning(f"pdfplumberæå–å¤±è´¥: {e}ï¼Œå°è¯•PyPDF2")
    
    # å¦‚æœpdfplumberå¤±è´¥ï¼Œå›é€€åˆ°PyPDF2
    try:
        with open(pdf_path, 'rb') as f:
            pdf_reader = PyPDF2.PdfReader(f)
            text_content = ""
            
            for page in pdf_reader.pages:
                page_text = page.extract_text()
                if page_text:
                    text_content += page_text + "\n"
            
            if text_content.strip():
                logger.info(f"ä½¿ç”¨PyPDF2æå–æ–‡æœ¬: {len(text_content)}å­—ç¬¦")
                return text_content.strip()
    
    except Exception as e:
        logger.error(f"PyPDF2æå–å¤±è´¥: {e}")
    
    return ""

def get_all_documents() -> List[Dict]:
    """
    è·å–æ‰€æœ‰æ–‡æ¡£ä¿¡æ¯
    
    Returns:
        List[Dict]: æ–‡æ¡£åˆ—è¡¨
    """
    try:
        with sqlite3.connect(DB_PATH) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute("""
                SELECT doc_id, original_filename, title, category, file_path
                FROM documents
                ORDER BY upload_time DESC
            """)
            
            documents = []
            for row in cursor.fetchall():
                documents.append({
                    'doc_id': row['doc_id'],
                    'filename': row['original_filename'],
                    'title': row['title'],
                    'category': row['category'],
                    'file_path': row['file_path']
                })
            
            return documents
    
    except Exception as e:
        logger.error(f"è·å–æ–‡æ¡£åˆ—è¡¨å¤±è´¥: {e}")
        return []

def update_document_chunks(doc_id: str, new_content: str) -> bool:
    """
    æ›´æ–°æ–‡æ¡£åˆ†å—å†…å®¹
    
    Args:
        doc_id: æ–‡æ¡£ID
        new_content: æ–°çš„æ–‡æœ¬å†…å®¹
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸ
    """
    try:
        # ç®€å•åˆ†å—ï¼ˆä¸åŸç³»ç»Ÿä¿æŒä¸€è‡´ï¼‰
        chunk_size = 800
        overlap = 100
        chunks = []
        
        start = 0
        while start < len(new_content):
            end = start + chunk_size
            chunk = new_content[start:end]
            
            if chunk.strip():
                chunks.append(chunk.strip())
            
            start = end - overlap
            if start >= len(new_content):
                break
        
        # æ›´æ–°æ•°æ®åº“
        with sqlite3.connect(DB_PATH) as conn:
            # åˆ é™¤æ—§çš„åˆ†å—
            conn.execute("DELETE FROM document_chunks WHERE doc_id = ?", (doc_id,))
            
            # æ’å…¥æ–°çš„åˆ†å—
            for i, chunk in enumerate(chunks):
                chunk_id = f"{doc_id}_chunk_{i}"
                conn.execute("""
                    INSERT INTO document_chunks 
                    (chunk_id, doc_id, chunk_index, content, vector_id, char_count)
                    VALUES (?, ?, ?, ?, ?, ?)
                """, (chunk_id, doc_id, i, chunk, -1, len(chunk)))
            
            conn.commit()
            
        logger.info(f"æ–‡æ¡£{doc_id}æ›´æ–°äº†{len(chunks)}ä¸ªåˆ†å—")
        return True
    
    except Exception as e:
        logger.error(f"æ›´æ–°æ–‡æ¡£åˆ†å—å¤±è´¥: {e}")
        return False

def fix_document_encoding(doc_info: Dict) -> bool:
    """
    ä¿®å¤å•ä¸ªæ–‡æ¡£çš„ç¼–ç é—®é¢˜
    
    Args:
        doc_info: æ–‡æ¡£ä¿¡æ¯
        
    Returns:
        bool: æ˜¯å¦æˆåŠŸä¿®å¤
    """
    doc_id = doc_info['doc_id']
    file_path = doc_info['file_path']
    title = doc_info['title']
    
    logger.info(f"æ­£åœ¨ä¿®å¤æ–‡æ¡£: {title} ({doc_id})")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not Path(file_path).exists():
        logger.error(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        return False
    
    # é‡æ–°æå–æ–‡æœ¬
    new_content = extract_pdf_text_improved(file_path)
    
    if not new_content:
        logger.error(f"æ— æ³•æå–æ–‡æœ¬å†…å®¹: {file_path}")
        return False
    
    # æ£€æŸ¥å†…å®¹æ˜¯å¦æœ‰æ”¹å–„
    if len(new_content) < 50:  # å†…å®¹å¤ªçŸ­å¯èƒ½æœ‰é—®é¢˜
        logger.warning(f"æå–çš„å†…å®¹å¯èƒ½ä¸å®Œæ•´: {len(new_content)}å­—ç¬¦")
    
    # æ˜¾ç¤ºæå–çš„å†…å®¹é¢„è§ˆ
    preview = new_content[:200] + "..." if len(new_content) > 200 else new_content
    logger.info(f"æå–å†…å®¹é¢„è§ˆ: {preview}")
    
    # æ›´æ–°æ•°æ®åº“
    success = update_document_chunks(doc_id, new_content)
    
    if success:
        logger.info(f"âœ… æ–‡æ¡£ä¿®å¤æˆåŠŸ: {title}")
    else:
        logger.error(f"âŒ æ–‡æ¡£ä¿®å¤å¤±è´¥: {title}")
    
    return success

def main():
    """
    ä¸»å‡½æ•°ï¼šä¿®å¤æ‰€æœ‰æ–‡æ¡£çš„ç¼–ç é—®é¢˜
    """
    logger.info("å¼€å§‹ä¿®å¤PDFæ–‡æ¡£ç¼–ç é—®é¢˜...")
    
    # è·å–æ‰€æœ‰æ–‡æ¡£
    documents = get_all_documents()
    
    if not documents:
        logger.warning("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ–‡æ¡£")
        return
    
    logger.info(f"æ‰¾åˆ°{len(documents)}ä¸ªæ–‡æ¡£ï¼Œå¼€å§‹ä¿®å¤...")
    
    success_count = 0
    failed_count = 0
    
    for doc_info in documents:
        try:
            if fix_document_encoding(doc_info):
                success_count += 1
            else:
                failed_count += 1
        except Exception as e:
            logger.error(f"ä¿®å¤æ–‡æ¡£æ—¶å‡ºé”™: {e}")
            failed_count += 1
        
        print("-" * 50)
    
    # æ€»ç»“
    logger.info(f"ä¿®å¤å®Œæˆï¼æˆåŠŸ: {success_count}, å¤±è´¥: {failed_count}")
    
    if success_count > 0:
        logger.info("å»ºè®®é‡æ–°å¯åŠ¨MCPæœåŠ¡ä»¥é‡å»ºå‘é‡ç´¢å¼•")
        print("\nğŸ”„ è¯·é‡æ–°å¯åŠ¨MCPæœåŠ¡ä»¥é‡å»ºå‘é‡ç´¢å¼•:")
        print("python knowledge_mcp.py")

if __name__ == "__main__":
    main()